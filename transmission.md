üíà
   "transmission" is a system for building legible gear level models and sharing them with others

   "making inside views kiss"

   Harrison Durland: "The core idea of an epistemic map is a virtual space for concept, entity, and link diagramming, with an emphasis on semantically-rich[1] relationships‚Äîfor example, claim X relies on assumption Y; claim X conflicts/contrasts with claim Y; study X relied on Y  dataset and Z methodology to make claim W; and so on"

   "It's hard to create and communicate gear level models of the world, especially across contexts (like across orgs)" - [[collective sense making chat post eag 20]]

related
   - [[ingota]]
   - [[dimensions of coherence]]
   - [[ai mansion explainer]]

Thoughts
   - üß© INTEROPERABILITY:     The key strength of this tool being the ability to marry inside views 

   - ü§ù REWARD SENSEMAKING:   What if it was easy and profitably to share the sense-making people do for forecasting?

   - "Cruxes, Forecasts, and Cost benefit distributions (CFCs) are good and should be more common"

   - Documents that can embed these are much easier to engage with than prose / an email

   - interesting how often people want to combine time later like "show me this later"

## üî® Rightness through discord
   - When one is trying to make sense of reality and how best to respond to it, one is doing two related tasks
      1. Building maps that represent reality (rightness through seeing)
      2. Resolving disagreement between maps, within ones self and with others (rightness through discord)

   - Tools that build agreement might be closely related to tools that enhance understanding / valuable to integrate in some way

   - Having a good representation of your map is helpful for resolving disagreements between them

   - There's a related intuition / problem / idea
   - That it's our ability to chunk and orchestrate thought that's important / needs improving...

## üî™ The Feynman Algorithm
   1. Write down the problem.
   2. Think real hard.
   3. Write down the solution

   "if you can get your gears down, the answer comes into view"

   "in some ways, sense-making is just fitting everything you need to into your head at once, so you can understand
   everything relevant about a situation at once, and what you need to do can become obvious
   
   what this looks like in practice, is sitting down and writing a lot
   and then identifying concepts that can be "chunked" and given a concept handle

   and then taking those concept handles and arranging them in relation to one another, so you can grok the situation in it's entirety
   like arranging a set of strings, and then strumming them"

## üí† Sharing Inside Views
   - "help people share their inner models"
   - "sharing your inside view"
   - "in most situations it's way too expensive to create an external representation of what an individual or group thinks" - Ozzie
   - you could have an ai parse it into a visual structure from a conversation?
   - this could be really helpful in AI safety

   - surfacing the explicit and implicit a-prior knowlege individually and collectibvly
   externalising an internal gears level model

## üß™ How Alex Demarsh sensemake
   "i dump some thoughts on a page and try to set up a causal model"
   "what kind of things are you trying to measure and how do they impact the outcomes you care about"
   "a lot of it is **just being reminded at the right time** so i can assess the quality of my mode. like come back to this thing and see if it matches your model of the world"
   
   - uses dendron

   - there are different facets of collective sense-making
      - is one tool appropriate for each stage?

   - theoretically you should be able to measure the impact of a tool
   - if you get a random selection of forcasting teams to use it

   "knowledge management should be seamless"

   isatero reference app
   
   "the intelligence process" (ingest, process, analyse, diseminate(?))

## ü§ñ AI Safety implication
   Idea: get a team together and specialise in summarising the work done by AI safety researchers

   üí† Seems Important
      hating write-ups
         - "i like talking and having the ideas, but writing them up is a pain"
         - asking questions feels important / very able to get things out onto the table

      novel ontology
         - when people do a lot of reasoning about a novel problem, they often have a lot of totally novel ideas
         - when you have a large set of novel ideas, it can become hard for others to understand them
         - and if someone doesn't understand your ideas about a problem, it's hard to work together

      inter-dependency
         - one dynamic here, is that ideas often relate to one another
         - they build on each other, or give each other context, or only make sense in reference to other ideas
         - so if you have a large body of novel ideas, it can be hard to gain entry at all, because you need to hold a bunch of stuff that makes no sense until it's contextualised

      forgetting to say it's wrong
         - there's this thing that happens, when someone is questioning
         - and in the back of my mind, i'm like "i know why they're confused, this thing doesn't actually bottom out on the left side i'm still working on that"
         - and it's not that you're hiding it it's that... idk you just kind of, forget? to say it? or there isn't time maybe?
         - and either they push until you reach it, or i have the presence of mind to say "you're not crazy, i see what you're saying this part is fuzzy"

      the project
         - going around to ai researchers, and being like "hey, could we take like 3 days to plumb your brain?"
         - we have a great track record of communicating esoteric webs of thought, and we want to "do you", make you legible
         - this would help them, explaining to a team of people skilled at groking arcane ontologies with a large negative capacity who's done as much prior reading as possible
         - like our world in data for AI ontologies
         - would also help AI researchers better understand one another and thus communicate 

   surface and formalise the low level intuitions their views sit on

   researchers > communicators > engineers pipeline

   scott alexandars project around that discord chat, making things legible

   "Although I am not involved in AI research, I occasionally read some of the less-technical discussions/analyses, and it definitely seems like there is a lot of disagreement and uncertainty on big questions like fast takeoff probability and AI alignment probability. Thus, it seems quite plausible that such a system as I am describing could help improve community understanding by:

   Breaking down big questions by looking at smaller, related questions (e.g., ‚Äúwhen will small-sample learning be effective‚Äù, ‚Äúhow strong will market incentives vs. market failures (e.g., nonexcludability of findings) be for R&D/investment‚Äù, ‚Äúhow feasible and useful will neural mapping be‚Äù);

   Sharing/aggregating individual ideas in a compact format‚Äîand making it easier to find those ideas when looking for, e.g., ‚Äúwhat are the counterarguments to claim X", "what are the criticisms of study Y";

   Highlighting the logical relationships/dependencies between studies, arguments, assumptions, etc. In particular, this could be valuable:
      If some study or dataset is found to be flawed in some way, and there was a way to flag any claims that were marked as relying on that study or dataset;
      To identify what are some of the most widespread assumptions or disputed claims in a field (which might be useful for deciding where one should focus their research)."

   people
      head of ai at tesla
      chris olaf
      rich sutton at deep mind
      adam shuri episimology
      camnon rock epistimological foundations

Next Steps
   - Funding public sense-makers to externalise their inside views using the tool
      - zvi?
